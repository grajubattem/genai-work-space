{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEOBYPDjIpKU",
        "outputId": "4883272a-0e0d-4f32-b34d-c45eb59e8f10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Sentence: What is the STEP by step guide to invest In share market in india?\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "Lowered Sentence: what is the step by step guide to invest in share market in india?\n"
          ]
        }
      ],
      "source": [
        "#  Lower case\n",
        "\n",
        "sentence=\"What is the STEP by step guide to invest In share market in india?\"\n",
        "sentence_lower=str(sentence).lower()\n",
        "print(\"Original Sentence:\",sentence)\n",
        "print(\"--\"*60)\n",
        "print(\"Lowered Sentence:\",sentence_lower)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "s9rbYB72J2hi",
        "outputId": "d51c2cc9-7a50-41b4-8fb4-c6345327f4a3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import string\n",
        "punc = string.punctuation\n",
        "punc\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkdly_GcKQg3",
        "outputId": "f679d8d4-ae8b-44fe-9086-c262167139af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', 'Everyone,', 'this', 'is', 'team', 'Data', 'Dynamos', 'We', 'are', 'got', 'an', 'project', 'of', 'Quora']\n"
          ]
        }
      ],
      "source": [
        "sentence=\"Hello Everyone, this is team Data Dynamos ! We are got an project of Quora\"\n",
        "without_punc=[word for word in sentence.split(\" \") if word not in list(punc)]\n",
        "print(without_punc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMzdaC5fLmF7",
        "outputId": "4510a6dc-9527-4f32-dbe5-b83ebf679c02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello Guys How Are You\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "# Remove HTML tags\n",
        "sentence='''<h3 style=\"color:red; font-family:Arial Black\">Hello Guys How Are You</h3>'''\n",
        "clean_sentence=re.sub(\"<.*?>\",\"\",sentence)\n",
        "print(clean_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueIqOkOuMlyb",
        "outputId": "64db2111-0220-42b6-9d75-af7556ba620f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I visited \n"
          ]
        }
      ],
      "source": [
        "# Removal of URLs\n",
        "sentence=\"I visited https://github.com/surajh8596/NLP-Sentiment-Analysis-/tree/main/\"\n",
        "clean_sentence=re.sub(\"(http|https|www)\\S+\",\"\",sentence)\n",
        "print(clean_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voY-sMC0NMYI",
        "outputId": "456f2cfe-f093-48a7-9ea5-64193aed2c82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hi Team Data Dynamos, How is your project going on ?\n"
          ]
        }
      ],
      "source": [
        "# Remove Extra Spaces\n",
        "\n",
        "sentence=\"Hi Team     Data Dynamos,    How is your     project     going on ?\"\n",
        "clean_sentence=re.sub(\" +\",\" \",sentence)\n",
        "print(clean_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipykernel in c:\\users\\graju\\anaconda3\\lib\\site-packages (6.28.0)\n",
            "Collecting ipykernel\n",
            "  Downloading ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: comm>=0.1.1 in c:\\users\\graju\\anaconda3\\lib\\site-packages (from ipykernel) (0.2.1)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\graju\\anaconda3\\lib\\site-packages (from ipykernel) (1.6.7)\n",
            "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\graju\\anaconda3\\lib\\site-packages (from ipykernel) (8.27.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\graju\\anaconda3\\lib\\site-packages (from ipykernel) (8.6.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\graju\\anaconda3\\lib\\site-packages (from ipykernel) (5.7.2)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\graju\\anaconda3\\lib\\site-packages (from ipykernel) (0.1.6)\n",
            "Requirement already satisfied: nest-asyncio in c:\\users\\graju\\anaconda3\\lib\\site-packages (from ipykernel) (1.6.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\graju\\anaconda3\\lib\\site-packages (from ipykernel) (24.1)\n",
            "Requirement already satisfied: psutil in c:\\users\\graju\\anaconda3\\lib\\site-packages (from ipykernel) (5.9.0)\n",
            "Requirement already satisfied: pyzmq>=24 in c:\\users\\graju\\anaconda3\\lib\\site-packages (from ipykernel) (25.1.2)\n",
            "Requirement already satisfied: tornado>=6.1 in c:\\users\\graju\\anaconda3\\lib\\site-packages (from ipykernel) (6.4.1)\n",
            "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\graju\\anaconda3\\lib\\site-packages (from ipykernel) (5.14.3)\n",
            "Requirement already satisfied: decorator in c:\\users\\graju\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in c:\\users\\graju\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.19.1)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\graju\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (3.0.43)\n",
            "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\graju\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (2.15.1)\n",
            "Requirement already satisfied: stack-data in c:\\users\\graju\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\graju\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\graju\\anaconda3\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\graju\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (3.10.0)\n",
            "Requirement already satisfied: pywin32>=300 in c:\\users\\graju\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (305.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\graju\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in c:\\users\\graju\\anaconda3\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.5)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\graju\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.16.0)\n",
            "Requirement already satisfied: executing in c:\\users\\graju\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.8.3)\n",
            "Requirement already satisfied: asttokens in c:\\users\\graju\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.0.5)\n",
            "Requirement already satisfied: pure-eval in c:\\users\\graju\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.2.2)\n",
            "Downloading ipykernel-6.29.5-py3-none-any.whl (117 kB)\n",
            "Installing collected packages: ipykernel\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 6.28.0\n",
            "    Uninstalling ipykernel-6.28.0:\n",
            "      Successfully uninstalled ipykernel-6.28.0\n",
            "Successfully installed ipykernel-6.29.5\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -U ipykernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S6JhumdNhaD",
        "outputId": "1d4cd629-9326-42b2-818d-a44d21396923"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "Downloading pyahocorasick-2.1.0-cp312-cp312-win_amd64.whl (39 kB)\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We have reached final step of our data science internship. We will meet you\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import contractions\n",
        "sentence=\"We've reached final step of our data science internship. We'll meet u\"\n",
        "\n",
        "clear_sentence=contractions.fix(sentence)\n",
        "print(clear_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPnAPDS6Oojf",
        "outputId": "6dd167c2-5c66-4727-924e-537643a5b0fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Our Team name is Team Data Dynamos and we have selected Quora question.', 'Team is good in performance']\n"
          ]
        }
      ],
      "source": [
        "# Sentence Toxenization\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "sentence='Our Team name is Team Data Dynamos and we have selected Quora question. Team is good in performance'\n",
        "\n",
        "tokens=sent_tokenize(sentence)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXOQC9WXPG2J",
        "outputId": "6e6a14e9-651c-4945-d83c-75a273017453"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\graju\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Our Team name is Team Data Dynamos and we have selected Quora question.', 'Team is good in performance']\n"
          ]
        }
      ],
      "source": [
        "# Sentence Toxenization\n",
        " \n",
        "from nltk.tokenize import sent_tokenize\n",
        "sentence='Our Team name is Team Data Dynamos and we have selected Quora question. Team is good in performance'\n",
        " \n",
        "tokens=sent_tokenize(sentence)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Our', 'Team', 'name', 'is', 'Team', 'Data', 'Dynamos', 'and', 'we', 'have', 'selected', 'Quora', 'question']\n"
          ]
        }
      ],
      "source": [
        "# Word TOkenization with Split function\n",
        " \n",
        "sentence='Our Team name is Team Data Dynamos and we have selected Quora'\n",
        "\n",
        "tokens=sentence.split(\" \")\n",
        " \n",
        "# Word TOkenization using word_tokenizer\n",
        " \n",
        "from nltk.tokenize import word_tokenize\n",
        " \n",
        "sentence='Our Team name is Team Data Dynamos and we have selected Quora question'\n",
        " \n",
        "tokens=word_tokenize(sentence)\n",
        "\n",
        "print(tokens)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\graju\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\graju\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\graju\\AppData\\Roaming\\nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')  # For tokenization\n",
        "nltk.download('stopwords')  # For stopwords\n",
        "nltk.download('wordnet')  # For lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stemming\n",
        " \n",
        "from nltk.stem import PorterStemmer\n",
        "porter=PorterStemmer()\n",
        " \n",
        "sentence=\"Connect Connection Connections Connecting Connected Connects Connectings\"\n",
        "porter_stem=[porter.stem(word)for word in sentence.split(\" \")]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
